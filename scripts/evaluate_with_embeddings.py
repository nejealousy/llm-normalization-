import json
import csv
from sklearn.metrics import precision_score, recall_score, f1_score
from sentence_transformers import SentenceTransformer, util
import numpy as np
import os
import re

# --- –ü–∞—Ä–∞–º–µ—Ç—Ä—ã ---
THRESHOLD = 0.001
SYNONYMS = {
    "–≤–∏–¥ –ø—Ä–æ–¥—É–∫—Ü–∏–∏": ["—Ç–∏–ø –ø—Ä–æ–¥—É–∫—Ç–∞", "—Ç–æ–≤–∞—Ä", "–ø—Ä–æ–¥—É–∫—Ü–∏—è", "—Ç–∏–ø", "–∫–∞—Ç–µ–≥–æ—Ä–∏—è"],
    "–±—Ä–µ–Ω–¥": ["–º–∞—Ä–∫–∞", "–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å", "–±—Ä–µ–Ω–¥", "–±—Ä–µ–Ω–¥ —Ç–æ–≤–∞—Ä–∞", "–∫–æ–º–ø–∞–Ω–∏—è", "–∏–∑–≥–æ—Ç–æ–≤–∏—Ç–µ–ª—å", "information –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª–µ", "–±—Ä–µ–Ω–¥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—è"],
    "–º–æ–¥–µ–ª—å": ["–Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏", "–º–æ–¥–µ–ª—å", "–∞—Ä—Ç–∏–∫—É–ª", "–∫–æ–¥ –º–æ–¥–µ–ª–∏", "–Ω–æ–º–µ—Ä –º–æ–¥–µ–ª–∏", "–Ω–æ–º–µ—Ä –∞—Ä—Ç–∏–∫—É–ª–∞", "–º–æ–¥–µ–ª—å —Ç–æ–≤–∞—Ä–∞", "—Å–µ—Ä–∏—è"],
    "—Ç–∏–ø": ["—Ç–∏–ø", "—Ç–∏–ø –∞–º–æ—Ä—Ç–∏–∑–∞—Ç–æ—Ä–∞", "—Ç–∏–ø –±–∞—Ç–∞—Ä–µ–∏", "—Ç–∏–ø —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞", "—Ç–∏–ø –∏–∑–¥–µ–ª–∏—è", "–≤–∞—Ä–∏–∞–Ω—Ç"],
    "—Ä–∞–∑–º–µ—Ä": ["—Ä–∞–∑–º–µ—Ä", "–≥–∞–±–∞—Ä–∏—Ç", "–¥–ª–∏–Ω–∞", "—à–∏—Ä–∏–Ω–∞", "–≤—ã—Å–æ—Ç–∞", "—Ç–æ–ª—â–∏–Ω–∞", "–¥–∏–∞–º–µ—Ç—Ä", "–≤–µ–ª–∏—á–∏–Ω–∞", "–æ–±—ä–µ–º"],
    "—Ü–≤–µ—Ç": ["—Ü–≤–µ—Ç", "–æ—Ç—Ç–µ–Ω–æ–∫", "–æ–∫—Ä–∞—Å–∫–∞"],
    "–≤–µ—Å": ["–≤–µ—Å", "–º–∞—Å—Å–∞", "weight"],
    "–º–∞—Ç–µ—Ä–∏–∞–ª": ["–º–∞—Ç–µ—Ä–∏–∞–ª", "—Å–æ—Å—Ç–∞–≤", "–æ—Å–Ω–æ–≤–∞", "–º–∞—Ç–µ—Ä–∏–∞–ª –∏–∑–¥–µ–ª–∏—è"],
    "–ø–æ–∫—Ä—ã—Ç–∏–µ": ["–ø–æ–∫—Ä—ã—Ç–∏–µ", "–ø–æ–∫—Ä—ã—Ç–∏–µ –∏–∑–¥–µ–ª–∏—è", "—Ç–∏–ø –ø–æ–∫—Ä—ã—Ç–∏—è"],
    "–∫–ª–∞—Å—Å –ø—Ä–æ—á–Ω–æ—Å—Ç–∏": ["–∫–ª–∞—Å—Å –ø—Ä–æ—á–Ω–æ—Å—Ç–∏", "–ø—Ä–æ—á–Ω–æ—Å—Ç—å", "—Å—Ç–∞–Ω–¥–∞—Ä—Ç –ø—Ä–æ—á–Ω–æ—Å—Ç–∏"],
    "—Å—Ç–∞–Ω–¥–∞—Ä—Ç": ["—Å—Ç–∞–Ω–¥–∞—Ä—Ç", "—Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç", "–Ω–æ—Ä–º–∞", "–ì–û–°–¢", "–¢–£", "ISO"],
    "–º–æ—â–Ω–æ—Å—Ç—å": ["–º–æ—â–Ω–æ—Å—Ç—å", "–º–æ—â–Ω–æ—Å—Ç—å –¥–≤–∏–≥–∞—Ç–µ–ª—è", "—ç–Ω–µ—Ä–≥–∏—è", "–≤—Ç", "–≤–∞—Ç—Ç"],
    "–µ–º–∫–æ—Å—Ç—å": ["–µ–º–∫–æ—Å—Ç—å", "–æ–±—ä–µ–º", "–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å", "capacity"],
    "–Ω–∞–ø—Ä—è–∂–µ–Ω–∏–µ": ["–Ω–∞–ø—Ä—è–∂–µ–Ω–∏–µ", "–≤–æ–ª—å—Ç–∞–∂", "–≤–æ–ª—å—Ç"],
    "—Å–∫–æ—Ä–æ—Å—Ç—å": ["—Å–∫–æ—Ä–æ—Å—Ç—å", "—á–∞—Å—Ç–æ—Ç–∞", "–æ–±–æ—Ä–æ—Ç—ã", "rpm"],
    "–¥–∞–≤–ª–µ–Ω–∏–µ": ["–¥–∞–≤–ª–µ–Ω–∏–µ", "—Ä–∞–±–æ—á–µ–µ –¥–∞–≤–ª–µ–Ω–∏–µ", "–º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –¥–∞–≤–ª–µ–Ω–∏–µ", "—Ä—É"],
    "—Ç–∏–ø –±–∞—Ç–∞—Ä–µ–∏": ["—Ç–∏–ø –±–∞—Ç–∞—Ä–µ–∏", "—Ö–∏–º–∏—á–µ—Å–∫–∏–π —Å–æ—Å—Ç–∞–≤", "—Ç–∏–ø –∞–∫–∫—É–º—É–ª—è—Ç–æ—Ä–∞"],
    "–∑–∞—â–∏—Ç–∞": ["–∑–∞—â–∏—Ç–∞", "–∫–ª–∞—Å—Å –∑–∞—â–∏—Ç—ã", "ip –∫–ª–∞—Å—Å", "—Å—Ç–µ–ø–µ–Ω—å –∑–∞—â–∏—Ç—ã"],
    "–º–æ–¥–µ–ª—å –¥–≤–∏–≥–∞—Ç–µ–ª—è": ["–º–æ–¥–µ–ª—å –¥–≤–∏–≥–∞—Ç–µ–ª—è", "—Ç–∏–ø –¥–≤–∏–≥–∞—Ç–µ–ª—è"],
    "—Å—Ä–æ–∫ —Å–ª—É–∂–±—ã": ["—Å—Ä–æ–∫ —Å–ª—É–∂–±—ã", "–≥–∞—Ä–∞–Ω—Ç–∏—è", "—Ä–µ—Å—É—Ä—Å"],
    "–æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏": ["–æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏", "—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏", "–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏", "–æ–ø–∏—Å–∞–Ω–∏–µ"],
    "–¥–ª–∏–Ω–∞": ["–¥–ª–∏–Ω–∞", "—Ä–∞–∑–º–µ—Ä –ø–æ –¥–ª–∏–Ω–µ"],
    "—à–∏—Ä–∏–Ω–∞": ["—à–∏—Ä–∏–Ω–∞", "—Ä–∞–∑–º–µ—Ä –ø–æ —à–∏—Ä–∏–Ω–µ"],
    "–≤—ã—Å–æ—Ç–∞": ["–≤—ã—Å–æ—Ç–∞", "—Ä–∞–∑–º–µ—Ä –ø–æ –≤—ã—Å–æ—Ç–µ"],
    # –î–æ–±–∞–≤—å —Å–≤–æ–∏ –≤–∞—Ä–∏–∞–Ω—Ç—ã
}


model = SentenceTransformer("sentence-transformers/paraphrase-mpnet-base-v2")

# --- –£—Ç–∏–ª–∏—Ç—ã ---
def normalize(text):
    if text is None:
        return "none"
    text = str(text).strip().lower()
    text = re.sub(r"[^\w\s\-.,]", "", text)
    return text

def are_similar(a, b):
    if normalize(a) == normalize(b):
        return True
    emb1 = model.encode(normalize(a), convert_to_tensor=True)
    emb2 = model.encode(normalize(b), convert_to_tensor=True)
    similarity = util.cos_sim(emb1, emb2).item()
    return similarity >= THRESHOLD

def get_key_synonyms(key):
    key = normalize(key)
    return [key] + SYNONYMS.get(key, [])

def match_keys(pred_keys, ref_key):
    ref_variants = get_key_synonyms(ref_key)
    for rk in ref_variants:
        for pk in pred_keys:
            if normalize(pk) == rk:
                return pk
    return None

# --- –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö ---
def load_data():
    with open("data/test_dataset.csv", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        raw_texts = []
        references = []
        for row in reader:
            raw_texts.append(row["raw_text"])
            references.append(json.loads(row["expected_attrs"]))

    with open("data/llm_outputs.json", encoding="utf-8") as f:
        preds_raw = json.load(f)

    predictions = [preds_raw.get(text, {}) for text in raw_texts]
    return raw_texts, references, predictions

# --- –û—Ü–µ–Ω–∫–∞ ---
def evaluate():
    raw_texts, references, predictions = load_data()

    tp, fp, fn = 0, 0, 0
    top_errors = []
    key_errors = {}

    for i, (ref, pred, text) in enumerate(zip(references, predictions, raw_texts)):
        pred_keys_used = set()
        for ref_key, ref_val in ref.items():
            matched_key = match_keys(pred.keys(), ref_key)
            if matched_key:
                pred_keys_used.add(matched_key)
                pred_val = pred[matched_key]
                if are_similar(ref_val, pred_val):
                    tp += 1
                else:
                    fn += 1
                    top_errors.append((f"–ù–µ–≤–µ—Ä–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ: {ref_key}", ref_val, pred_val, text))
                    key_errors[ref_key] = key_errors.get(ref_key, 0) + 1
            else:
                fn += 1
                top_errors.append((f"–û–∂–∏–¥–∞–µ—Ç—Å—è, –Ω–æ –Ω–µ –Ω–∞–π–¥–µ–Ω: {ref_key}", ref_val, None, text))
                key_errors[ref_key] = key_errors.get(ref_key, 0) + 1

        for pred_key in pred.keys():
            if pred_key not in pred_keys_used:
                fp += 1
                top_errors.append((f"–õ–∏—à–Ω–∏–π –∫–ª—é—á: {pred_key}", None, pred[pred_key], text))
                key_errors[pred_key] = key_errors.get(pred_key, 0) + 1

    precision = tp / (tp + fp) if tp + fp > 0 else 0.0
    recall = tp / (tp + fn) if tp + fn > 0 else 0.0
    f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0.0

    print("\nüìä –ú–µ—Ç—Ä–∏–∫–∏:")
    print(f"Precision: {precision:.3f}")
    print(f"Recall: {recall:.3f}")
    print(f"F1 Score: {f1:.3f}")

    print("\n‚ùå –¢–æ–ø –æ—à–∏–±–æ–∫:")
    for err_type, expected, predicted, text in top_errors[:10]:
        print(f"- {err_type} | –û–∂–∏–¥.: {expected} | –ü—Ä–µ–¥—Å–∫–∞–∑.: {predicted} | –¢–µ–∫—Å—Ç: {text[:50]}...")

    print("\nüìå –ß–∞—Å—Ç—ã–µ –æ—à–∏–±–∫–∏:")
    for key, count in sorted(key_errors.items(), key=lambda x: x[1], reverse=True)[:10]:
        print(f"- {key}: {count}")

if __name__ == "__main__":
    print("üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ...")
    print("‚öôÔ∏è –û—Ü–µ–Ω–∏–≤–∞–µ–º...")
    evaluate()
